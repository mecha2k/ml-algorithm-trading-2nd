# Topic Modeling: probabilistic LSA / Non-negative Matrix Factorization
# Probabilistic Latent Semantic Analysis (pLSA) takes a statistical perspective on LSA and creates a generative model
# to address the lack of theoretical underpinnings of LSA.
#
# pLSA explicitly models the probability each co-occurrence of documents d and words w described by the DTM as a
# mixture of conditionally independent multinomial distributions that involve topics t.
# The symmetric formulation of this generative process of word-document co-occurrences assumes both words and documents
# are generated by the latent topic class, whereas the asymmetric model assumes the topics are selected given the
# document, and words result in a second step given the topic.
# $$P(w, d) = \underbrace{\sum_tP(d\mid t)P(w\mid t)}_{\text{symmetric}}=\underbrace{P(d) \sum_tP(t\mid d)P(w\mid t)}
# _{\text{asymmetric}}$$
# The number of topics is a hyperparameter chosen prior to training and is not learned from the data.
# The benefits of using a probability model is that we can now compare models by evaluating the probability they assign
# to new documents given the parameters learned during training.

from pathlib import Path
from random import randint
import numpy as np
import pandas as pd

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import NMF
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
import seaborn as sns

np.random.seed(seed=42)
sns.set_style("whitegrid")
plt.rcParams["figure.dpi"] = 300
plt.rcParams["font.size"] = 14
pd.options.display.float_format = "{:,.2f}".format

DATA_DIR = Path("../data")

if __name__ == "__main__":
    path = DATA_DIR / "bbc"
    files = sorted(list(path.glob("**/*.txt")))
    doc_list = []
    for i, file in enumerate(files):
        with open(str(file), encoding="latin1") as f:
            topic = file.parts[-2]
            lines = f.readlines()
            heading = lines[0].strip()
            body = " ".join([l.strip() for l in lines[1:]])
            doc_list.append([topic.capitalize(), heading, body])

    ### Convert to DataFrame
    docs = pd.DataFrame(doc_list, columns=["Category", "Heading", "Article"])
    docs.info()

    ## Create Train & Test Sets
    train_docs, test_docs = train_test_split(
        docs, stratify=docs.Category, test_size=50, random_state=42
    )
    print(train_docs.shape, test_docs.shape)
    print(pd.Series(test_docs.Category).value_counts())

    ### Vectorize train & test sets
    vectorizer = TfidfVectorizer(max_df=0.2, min_df=0.01, stop_words="english")

    train_dtm = vectorizer.fit_transform(train_docs.Article)
    words = vectorizer.get_feature_names_out()
    print(train_dtm)
    test_dtm = vectorizer.transform(test_docs.Article)
    print(test_dtm)

    ### Get token counts
    train_token_count = train_dtm.sum(0).A.squeeze()
    tokens = vectorizer.get_feature_names_out()
    word_count = pd.Series(train_token_count, index=tokens).sort_values(ascending=False)
    print(word_count.head(10))

    ## probabilistic Latent Semantic Analysis
    ### Implementation using Non-Negative Matrix Factorization
    # pLSI [has been shown](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.8839&rep=rep1&type=pdf) to be
    # equivalent to Non-Negative Matrix Factorization with Kullback-Leibler Divergence objective.
    # pLSI is equivalent to Non-Negative Matrix Factorization using a Kullback-Leibler Divergence objective
    # (see references on GitHub). Hence, we can use the [sklearn.decomposition.NMF]
    # (http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) class to implement this model,
    # following closely the LSA example.

    n_components = 5
    topic_labels = ["Topic {}".format(i) for i in range(1, n_components + 1)]

    # Using the same train-test split of the DTM produced by the TfidfVectorizer, we fit pLSA like so:
    nmf = NMF(
        init="nndsvda",
        n_components=n_components,
        random_state=42,
        solver="mu",
        beta_loss="kullback-leibler",
        max_iter=1000,
    )
    nmf.fit(train_dtm)

    # We get a measure of the reconstruction error that is a substitute for the explained variance measure for LSI:
    print(nmf.reconstruction_err_)

    ### Explore Topics
    train_doc_topics = nmf.transform(train_dtm)
    print(train_doc_topics.shape)

    i = randint(0, len(train_docs))
    (train_docs.iloc[i, :2].append(pd.Series(train_doc_topics[i], index=topic_labels)))
    train_result = pd.DataFrame(
        data=train_doc_topics, columns=topic_labels, index=train_docs.Category
    )
    test_eval = pd.DataFrame(
        data=nmf.transform(test_dtm), columns=topic_labels, index=test_docs.Category
    )

    # Due to its probabilistic nature, pLSA produces only positive topic weights that result in more straightforward
    # topic-category relationships for the test and training sets:
    result = pd.melt(
        train_result.assign(Data="Train").append(test_eval.assign(Data="Test")).reset_index(),
        id_vars=["Data", "Category"],
        var_name="Topic",
        value_name="Weight",
    )
    result = pd.melt(
        train_result.assign(Data="Train").append(test_eval.assign(Data="Test")).reset_index(),
        id_vars=["Data", "Category"],
        var_name="Topic",
        value_name="Weight",
    )

    g = sns.catplot(
        x="Category",
        y="Weight",
        hue="Topic",
        row="Data",
        kind="bar",
        data=result,
        height=3,
        aspect=4,
    )
    g.savefig("images/02-01.png")

    ### Most important words by topic
    # We can also see that the word lists that describe each topic begin to make more sense, e.g. the ‘Entertainment’
    # category is most directly associated with Topic 4 that includes the words ‘film’, ‘start’, etc.
    topics = pd.DataFrame(nmf.components_.T, index=tokens, columns=topic_labels)
    print(topics.loc[word_count.head(10).index])

    fig, ax = plt.subplots(figsize=(12, 4))
    top_words, top_vals = pd.DataFrame(), pd.DataFrame()
    for topic, words_ in topics.items():
        top10 = words_.nlargest(10).index
        vals = words_.loc[top10].values
        top_vals[topic] = vals
        top_words[topic] = top10.tolist()
    sns.heatmap(
        pd.DataFrame(top_vals),
        annot=top_words,
        fmt="",
        center=0,
        cmap=sns.diverging_palette(0, 255, sep=1, n=256),
        ax=ax,
    )
    ax.set_title("Top Words per Topic")
    fig.tight_layout()
    fig.savefig("images/02-02.png")

    topics = pd.DataFrame(nmf.components_.T, index=words, columns=topic_labels)
    print(topics.head())

    top_words = {}
    for topic, words_ in topics.items():
        top_words[topic] = words_.nlargest(10).index.tolist()
    print(pd.DataFrame(top_words))
