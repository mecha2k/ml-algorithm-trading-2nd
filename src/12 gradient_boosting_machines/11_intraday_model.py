# Intraday Strategy, Part 2: Model Training & Signal Evaluation
# In this notebook, we load the high-quality NASDAQ100 minute-bar trade-and-quote data generously provided by [Algoseek]
# (https://www.algoseek.com/) (available [here](https://www.algoseek.com/ml4t-book-data.html)) and use the features
# engineered in the last notebook to train gradient boosting model that predicts the returns for the NASDAQ100 stocks
# over the next 1-minute bar.
# > Note that we will assume throughout that we can always buy (sell) at the first (last) trade price for a given bar
# at no cost and without market impact. This does certainly not reflect market reality, and is rather due to the
# challenges of simulating a trading strategy at this much higher intraday frequency in a realistic manner using
# open-source tools.
# Note also that this section has slightly changed from the version published in the book to permit replication
# using the Algoseek data sample.

import sys, os
from pathlib import Path
from time import time
from tqdm import tqdm

import numpy as np
import pandas as pd

from scipy.stats import spearmanr
import lightgbm as lgb

import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
import seaborn as sns
import warnings

np.random.seed(42)
idx = pd.IndexSlice
sns.set_style("darkgrid")
plt.rcParams["figure.dpi"] = 300
plt.rcParams["font.size"] = 14
warnings.filterwarnings("ignore")
pd.options.display.float_format = "{:,.2f}".format

nasdaq_path = Path("../data/nasdaq100")


def format_time(t):
    """Return a formatted time string 'HH:MM:SS
    based on a numeric time() value"""
    m, s = divmod(t, 60)
    h, m = divmod(m, 60)
    return f"{h:0>2.0f}:{m:0>2.0f}:{s:0>2.0f}"


## Model Training
### Helper functions
class MultipleTimeSeriesCV:
    """Generates tuples of train_idx, test_idx pairs
    Assumes the MultiIndex contains levels 'symbol' and 'date'
    purges overlapping outcomes"""

    def __init__(
        self,
        n_splits=3,
        train_period_length=126,
        test_period_length=21,
        lookahead=None,
        date_idx="date",
        shuffle=False,
    ):
        self.n_splits = n_splits
        self.lookahead = lookahead
        self.test_length = test_period_length
        self.train_length = train_period_length
        self.shuffle = shuffle
        self.date_idx = date_idx

    def split(self, X, y=None, groups=None):
        unique_dates = X.index.get_level_values(self.date_idx).unique()
        days = sorted(unique_dates, reverse=True)
        split_idx = []
        for i in range(self.n_splits):
            test_end_idx = i * self.test_length
            test_start_idx = test_end_idx + self.test_length
            train_end_idx = test_start_idx + self.lookahead - 1
            train_start_idx = train_end_idx + self.train_length + self.lookahead - 1
            split_idx.append([train_start_idx, train_end_idx, test_start_idx, test_end_idx])

        dates = X.reset_index()[[self.date_idx]]
        for train_start, train_end, test_start, test_end in split_idx:

            train_idx = dates[
                (dates[self.date_idx] > days[train_start])
                & (dates[self.date_idx] <= days[train_end])
            ].index
            test_idx = dates[
                (dates[self.date_idx] > days[test_start]) & (dates[self.date_idx] <= days[test_end])
            ].index
            if self.shuffle:
                np.random.shuffle(list(train_idx))
            yield train_idx.to_numpy(), test_idx.to_numpy()

    def get_n_splits(self, X, y, groups=None):
        return self.n_splits


if __name__ == "__main__":
    deciles = np.arange(0.1, 1, 0.1)
    # where we stored the features engineered in the previous notebook
    data_store = nasdaq_path / "algoseek.h5"

    # where we'll store the model results
    result_store = nasdaq_path / "intra_day.h5"

    # here we save the trained models
    model_path = Path("../data/ch12/models/intraday")
    if not model_path.exists():
        model_path.mkdir(parents=True)

    ## Load Model Data
    data = pd.read_hdf(data_store, "model_data2")
    data.info(show_counts=True)
    print(data.sample(frac=0.1).describe(percentiles=np.arange(0.1, 1, 0.1)))

    def get_fi(model):
        fi = model.feature_importance(importance_type="gain")
        return pd.Series(fi / fi.sum(), index=model.feature_name())

    ### Categorical Variables
    data["stock_id"] = pd.factorize(data.index.get_level_values("ticker"), sort=True)[0]
    categoricals = ["stock_id"]

    ### Custom Metric
    def ic_lgbm(preds, train_data):
        """Custom IC eval metric for lightgbm"""
        is_higher_better = True
        return "ic", spearmanr(preds, train_data.get_label())[0], is_higher_better

    ### Cross-validation setup
    DAY = 390  # number of minute bars in a trading day of 6.5 hrs (9:30 - 15:59)
    MONTH = 21  # trading days

    def get_cv(n_splits=23):
        return MultipleTimeSeriesCV(
            n_splits=n_splits,
            lookahead=1,
            test_period_length=MONTH * DAY,  # test for 1 month
            train_period_length=12 * MONTH * DAY,  # train for 1 year
            date_idx="date_time",
        )

    # Show train/validation periods:
    for i, (train_idx, test_idx) in enumerate(get_cv().split(X=data)):
        train_dates = data.iloc[train_idx].index.unique("date_time")
        test_dates = data.iloc[test_idx].index.unique("date_time")
        print(train_dates.min(), train_dates.max(), test_dates.min(), test_dates.max())

    ### Train model
    label = sorted(data.filter(like="fwd").columns)
    features = data.columns.difference(label).tolist()
    label = label[0]

    params = dict(
        objective="regression",
        metric=["rmse"],
        device="gpu",
        max_bin=63,
        gpu_use_dp=False,
        num_leaves=16,
        min_data_in_leaf=500,
        feature_fraction=0.8,
        verbose=-1,
    )

    num_boost_round = 250
    cv = get_cv(n_splits=23)  # we have enough data for 23 different test periods

    def get_scores(result):
        return pd.DataFrame({"train": result["training"]["ic"], "valid": result["valid_1"]["ic"]})

    # The following model-training loop will take more than 10 hours to run and also consumes substantial memory.
    # If you run into resource constraints, you can modify the code, e.g., by:
    # 1. Only loading data required for one iteration.
    # 2. Shortening the training period to require less than one year.
    # You can also speed up the process by using fewer `n_splits`, which implies longer test periods.
    start = time()
    for fold, (train_idx, test_idx) in enumerate(cv.split(X=data), 1):
        # create lgb train set
        train_set = data.iloc[train_idx, :]
        lgb_train = lgb.Dataset(
            data=train_set.drop(label, axis=1),
            label=train_set[label],
            categorical_feature=categoricals,
        )

        # create lgb test set
        test_set = data.iloc[test_idx, :]
        lgb_test = lgb.Dataset(
            data=test_set.drop(label, axis=1),
            label=test_set[label],
            categorical_feature=categoricals,
            reference=lgb_train,
        )

        # train model
        evals_result = {}
        model = lgb.train(
            params=params,
            train_set=lgb_train,
            valid_sets=[lgb_train, lgb_test],
            feval=ic_lgbm,
            num_boost_round=num_boost_round,
            evals_result=evals_result,
            verbose_eval=50,
        )
        model.save_model((model_path / f"{fold:02}.txt").as_posix())

        # get train/valid ic scores
        scores = get_scores(evals_result)
        scores.to_hdf(result_store, f"ic/{fold:02}")

        # get feature importance
        fi = get_fi(model)
        fi.to_hdf(result_store, f"fi/{fold:02}")

        # generate validation predictions
        X_test = test_set.loc[:, model.feature_name()]
        y_test = test_set.loc[:, [label]]
        y_test["pred"] = model.predict(X_test)
        y_test.to_hdf(result_store, f"predictions/{fold:02}")

        # compute average IC per minute
        by_minute = y_test.groupby(test_set.index.get_level_values("date_time"))
        daily_ic = by_minute.apply(lambda x: spearmanr(x[label], x.pred)[0]).mean()
        print(f"\nFold: {fold:02} | {format_time(time()-start)} | IC per minute: {daily_ic:.2%}\n")

    ## Signal Evaluation
    with pd.HDFStore(result_store) as store:
        pred_keys = [k[1:] for k in store.keys() if k[1:].startswith("pred")]
        cv_predictions = pd.concat([store[k] for k in pred_keys]).sort_index()
    cv_predictions.info(show_counts=True)

    time_stamp = cv_predictions.index.get_level_values("date_time")
    dates = sorted(np.unique(time_stamp.date))

    # We have out-of-sample predictions for 484 days from February 2016 through December 2017:
    print(f"# Days: {len(dates)} | First: {dates[0]} | Last: {dates[-1]}")

    # We only use minutes with at least 100 predictions:
    n = cv_predictions.groupby("date_time").size()

    # There are ~700 periods, equivalent to a bit over a single trading day (0.67% of all periods in the sample), with
    # fewer than 100 predictions over the 23 test months:
    incomplete_minutes = n[n < 100].index
    print(f"{len(incomplete_minutes)} ({len(incomplete_minutes)/len(n):.2%})")

    cv_predictions = cv_predictions[~time_stamp.isin(incomplete_minutes)]
    cv_predictions.info(show_counts=True)

    ### Information Coefficient
    #### Across all periods
    ic = spearmanr(cv_predictions.fwd1min, cv_predictions.pred)[0]

    #### By minute
    # We are making new predictions every minute, so it makes sense to look at the average performance across all
    # short-term forecasts:
    minutes = cv_predictions.index.get_level_values("date_time")
    by_minute = cv_predictions.groupby(minutes)
    ic_by_minute = by_minute.apply(lambda x: spearmanr(x.fwd1min, x.pred)[0])

    minute_ic_mean = ic_by_minute.mean()
    minute_ic_median = ic_by_minute.median()
    print(
        f"\nAll periods: {ic:6.2%} | By Minute: {minute_ic_mean: 6.2%} (Median: {minute_ic_median: 6.2%})"
    )

    # Plotted as a five-day rolling average, we see that the IC was mostly below the out-of-sample period mean, and
    # increased during the last quarter of 2017 (as reflected in the validation results we observed while training
    # the model).

    ax = ic_by_minute.rolling(5 * 650).mean().plot(figsize=(14, 5), title="IC (5-day MA)", rot=0)
    ax.axhline(minute_ic_mean, ls="--", lw=1, c="k")
    ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: "{:.0%}".format(y)))
    ax.set_ylabel("Information Coefficient")
    ax.set_xlabel("")
    plt.tight_layout()
    plt.savefig("images/11-01.png")

    ### Vectorized backtest of a naive strategey: financial performance by signal quantile
    # Alphalens does not work with minute-data, so we need to compute our own signal performance measures.
    # Unfortunately, Zipline's Pipeline also doesn't work for minute-data and Backtrader takes a very long time with
    # such a large dataset. Hence, instead of an event-driven backtest of entry/exit rules as in previous examples,
    # we can only create a rough sketch of the financial performance of a naive trading strategy driven by the model's
    # predictions using vectorized backtesting (see Chapter 8 on the [ML4T workflow](../08_ml4t_workflow'). As we will
    # see below, this does not produce particularly helpful results.

    # This naive strategy invests in equal-weighted portfolios of the stocks in each decile under the following
    # assumptions (mentioned at the beginning of this notebook:
    # 1. Based on the predictions using inputs from the current and previous bars, we can enter positions at the first
    #    trade price in the following minute bar
    # 2. We exit all positions at the last price in that following minute bar
    # 3. There are no trading cost or market impact (slippage) of our trades (but we can check how sensitive
    #    the results would be).

    #### Average returns by minute bar and signal quantile
    # To this end, we compute the quintiles and deciles of the model's `fwd1min` predictions for each minute:
    by_minute = cv_predictions.groupby(minutes, group_keys=False)

    labels = list(range(1, 6))
    cv_predictions["quintile"] = by_minute.apply(
        lambda x: pd.qcut(x.pred, q=5, labels=labels).astype(int)
    )

    labels = list(range(1, 11))
    cv_predictions["decile"] = by_minute.apply(
        lambda x: pd.qcut(x.pred, q=10, labels=labels).astype(int)
    )
    cv_predictions.info(show_counts=True)

    #### Descriptive statistics of intraday returns by quintile and decile of model predictions
    # Next, we compute the average one-minute returns for each quintile / decile and minute.
    def compute_intraday_returns_by_quantile(predictions, quantile="quintile"):
        by_quantile = cv_predictions.reset_index().groupby(["date_time", quantile])
        return by_quantile.fwd1min.mean().unstack(quantile).sort_index()

    intraday_returns = {
        "quintile": compute_intraday_returns_by_quantile(cv_predictions),
        "decile": compute_intraday_returns_by_quantile(cv_predictions, quantile="decile"),
    }

    def summarize_intraday_returns(returns):
        summary = returns.describe(deciles)
        return pd.concat(
            [
                summary.iloc[:1].applymap(lambda x: f"{x:,.0f}"),
                summary.iloc[1:].applymap(lambda x: f"{x:.4%}"),
            ]
        )

    # The returns per minute, averaged over the 23-months period, increase by quintile/decile and range from -.3 (-.4)
    # to .27 (.37) basis points for the bottom and top quintile (decile), respectively. While this aligns with the
    # finding of a weakly positive rank correlation coefficient, it also suggests that such small gains are unlikely
    # to survive the impact of trading costs.
    summary = summarize_intraday_returns(intraday_returns["quintile"])
    print(summary)

    summary = summarize_intraday_returns(intraday_returns["decile"])
    print(summary)

    #### Cumulative Performance by Quantile
    # To simulate the performance of our naive strategy that trades all available stocks every minute, we simply assume
    # that we can reinvest (including potential gains/losses) every minute. To check for the sensitivity with respect
    # for trading cost, we can assume they are a constant number (fraction) of basis points, and subtract this number
    # from the minute-bar returns.
    def plot_cumulative_performance(returns, quantile="quintile", trading_costs_bp=0):
        """Plot average return by quantile (in bp) as well as cumulative return,
        both net of trading costs (provided as basis points; 1bp = 0.01%)
        """
        fig, axes = plt.subplots(figsize=(14, 4), ncols=2)
        sns.barplot(
            y="fwd1min",
            x=quantile,
            data=returns[quantile]
            .mul(10000)
            .sub(trading_costs_bp)
            .stack()
            .to_frame("fwd1min")
            .reset_index(),
            ax=axes[0],
        )
        axes[0].set_title(f"Avg. 1-min Return by Signal {quantile.capitalize()}")
        axes[0].set_ylabel("Return (bps)")
        axes[0].set_xlabel(quantile.capitalize())

        title = f"Cumulative Return by Signal {quantile.capitalize()}"
        (
            returns[quantile]
            .sort_index()
            .add(1)
            .sub(trading_costs_bp / 10000)
            .cumprod()
            .sub(1)
            .plot(ax=axes[1], title=title)
        )

        axes[1].yaxis.set_major_formatter(FuncFormatter(lambda y, _: "{:.0%}".format(y)))
        axes[1].set_xlabel("")
        axes[1].set_ylabel("Return")
        fig.suptitle(
            f"Average and Cumulative Performance (Net of Trading Cost: {trading_costs_bp:.2f}bp)"
        )
        fig.tight_layout()

    # Without trading costs, the compounding of even fairly small gains leads to extremely large cumulative profits for
    # the top quantile. However, these disappear as soon as we allow for minuscule trading costs that reduce the average
    # quantile return close to zero.

    ##### Without trading costs
    plot_cumulative_performance(intraday_returns, "quintile", trading_costs_bp=0)
    plt.savefig("images/11-02.png")

    plot_cumulative_performance(intraday_returns, "decile", trading_costs_bp=0)
    plt.savefig("images/11-03.png")

    ##### With extremely low trading costs
    # assuming costs of a fraction of a basis point, close to the average return of the top quantile
    plot_cumulative_performance(intraday_returns, "quintile", trading_costs_bp=0.2)
    plt.savefig("images/11-04.png")

    plot_cumulative_performance(intraday_returns, "decile", trading_costs_bp=0.3)
    plt.savefig("images/11-05.png")

    ### Feature Importance
    # We'll take a quick look at the features that most contributed to improving the IC across the 23 folds:
    with pd.HDFStore(result_store) as store:
        fi_keys = [k[1:] for k in store.keys() if k[1:].startswith("fi")]
        fi = pd.concat([store[k].to_frame(i) for i, k in enumerate(fi_keys, 1)], axis=1)

    # The top features from a conventional feature importance perspective are the ticker, followed by NATR, minute of
    # the day, latest 1m return and the CCI:
    fi.mean(1).nsmallest(25).plot.barh(figsize=(12, 8), title="LightGBM Feature Importance (gain)")
    plt.tight_layout()
    plt.savefig("images/11-06.png")

    # Explore with greater accuracy and in more detail how feature values affect predictions using SHAP values as
    # demonstrated in various other notebooks in this Chapter and the appendix!

    ## Conclusion
    # We have seen that a relatively simple gradient boosting model is able to achieve fairly consistent predictive
    # performance that is significantly better than a random guess even on a very short horizon.
    # However, the resulting economic gains of our naive strategy of frequently buying/(short-)selling the top/bottome
    # quantiles are too small to overcome the inevitable transaction costs. On the one hand, this demonstrates the
    # challenges of extracting value from a predictive signal. On the other hand, it shows that we need a more
    # sophisticated backtesting platform so that we can even begin to design and evaluate a more sophisticated strategy
    # that requires far fewer trades to exploit the signal in our ML predictions.
    # In addition, we would also want to work on improving the model by adding more informative feature, e.g. based on
    # the quote/trade info contained in the Algoseek data, or by fine-tuning our model architecture and hyperparameter
    # settings.
