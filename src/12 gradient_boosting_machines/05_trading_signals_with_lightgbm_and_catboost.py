# Long-Short Strategy, Part 2: Trading signals with LightGBM and CatBoost
# In this section, we'll start designing, implementing, and evaluating a trading strategy for US equities driven by
# daily return forecasts produced by gradient boosting models.
# As in the previous examples, we'll lay out a framework and build a specific example that you can adapt to run your
# own experiments. There are numerous aspects that you can vary, from the asset class and investment universe to more
# granular aspects like the features, holding period, or trading rules. See, for example, the **Alpha Factor Library**
# in the [Appendix](../24_alpha_factor_library) for numerous additional features.
# We'll keep the trading strategy simple and only use a single ML signal; a real-life application will likely use
# multiple signals from different sources, such as complementary ML models trained on different datasets or with
# different lookahead or lookback periods. It would also use sophisticated risk management, from simple stop-loss
# to value-at-risk analysis.
# **Six notebooks** cover our workflow sequence:
# 1. `preparing_the_model_data` (this noteboook): we'll engineer a few simple features from the Quandl Wiki data
# 2. [trading_signals_with_lightgbm_and_catboost](05_trading_signals_with_lightgbm_and_catboost.ipynb): we tune
#      hyperparameters for LightGBM and CatBoost to select a model, using 2015/16 as our validation period.
# 3. [evaluate_trading_signals](06_evaluate_trading_signals.ipynb): we compare the cross-validation performance using
#      various metrics to select the best model.
# 4. [model_interpretation](07_model_interpretation.ipynb): we take a closer look at the drivers behind
#      the best model's predictions.
# 5. [making_out_of_sample_predictions](08_making_out_of_sample_predictions.ipynb): we generate predictions
#      for our out-of-sample test period 2017.
# 6. [backtesting_with_zipline](09_backtesting_with_zipline.ipynb): evaluate the historical performance of
#      a long-short strategy based on our predictive signals using Zipline.
#
# We'll subset the dataset created in the preceding notebook through the end of 2016 to cross-validate several model
# configurations for various lookback and lookahead windows, as well as different roll-forward periods and hyperparameters.
# Our approach to model selection will be similar to the one we used in the previous chapter and uses the custom
# `MultipleTimeSeriesCV` introduced in [Chapter 7, Linear Models â€“ From Risk Factors to Return Forecasts](../07_linear_models).

import sys, os
from time import time
from tqdm import tqdm
from pathlib import Path
from collections import defaultdict
from itertools import product

import numpy as np
import pandas as pd

import lightgbm as lgb
from catboost import Pool, CatBoostRegressor
from sklearn.linear_model import LinearRegression
from scipy.stats import spearmanr
from alphalens.tears import create_summary_tear_sheet, create_full_tear_sheet
from alphalens.utils import get_clean_factor_and_forward_returns

import matplotlib.pyplot as plt
import seaborn as sns
import warnings


np.random.seed(42)
idx = pd.IndexSlice
sns.set_style("whitegrid")
plt.rcParams["figure.dpi"] = 300
plt.rcParams["font.size"] = 14
warnings.filterwarnings("ignore")
pd.options.display.float_format = "{:,.2f}".format

results_path = Path("../data/ch12", "us_stocks")
if not results_path.exists():
    results_path.mkdir(parents=True)


def format_time(t):
    """Return a formatted time string 'HH:MM:SS
    based on a numeric time() value"""
    m, s = divmod(t, 60)
    h, m = divmod(m, 60)
    return f"{h:0>2.0f}:{m:0>2.0f}:{s:0>2.0f}"


class MultipleTimeSeriesCV:
    """Generates tuples of train_idx, test_idx pairs
    Assumes the MultiIndex contains levels 'symbol' and 'date'
    purges overlapping outcomes"""

    def __init__(
        self,
        n_splits=3,
        train_period_length=126,
        test_period_length=21,
        lookahead=None,
        date_idx="date",
        shuffle=False,
    ):
        self.n_splits = n_splits
        self.lookahead = lookahead
        self.test_length = test_period_length
        self.train_length = train_period_length
        self.shuffle = shuffle
        self.date_idx = date_idx

    def split(self, X, y=None, groups=None):
        unique_dates = X.index.get_level_values(self.date_idx).unique()
        days = sorted(unique_dates, reverse=True)
        split_idx = []
        for i in range(self.n_splits):
            test_end_idx = i * self.test_length
            test_start_idx = test_end_idx + self.test_length
            train_end_idx = test_start_idx + self.lookahead - 1
            train_start_idx = train_end_idx + self.train_length + self.lookahead - 1
            split_idx.append([train_start_idx, train_end_idx, test_start_idx, test_end_idx])

        dates = X.reset_index()[[self.date_idx]]
        for train_start, train_end, test_start, test_end in split_idx:

            train_idx = dates[
                (dates[self.date_idx] > days[train_start])
                & (dates[self.date_idx] <= days[train_end])
            ].index
            test_idx = dates[
                (dates[self.date_idx] > days[test_start]) & (dates[self.date_idx] <= days[test_end])
            ].index
            if self.shuffle:
                np.random.shuffle(list(train_idx))
            yield train_idx.to_numpy(), test_idx.to_numpy()

    def get_n_splits(self, X, y, groups=None):
        return self.n_splits


if __name__ == "__main__":
    YEAR = 252
    # We select the train and validation sets, and identify labels and features:
    data = (
        pd.read_hdf("../data/12_data.h5", "model_data").sort_index().loc[idx[:, :"2016"], :]
    )  # train & validation period
    data.info(show_counts=True)

    labels = sorted(data.filter(like="_fwd").columns)
    features = data.columns.difference(
        labels
    ).tolist()  # features are columns not containing '_fwd'

    ## Model Selection: Lookback, lookahead and roll-forward periods
    tickers = data.index.get_level_values("symbol").unique()

    # We may want to predict 1, 5 or 21-day returns:
    lookaheads = [1, 5, 21]
    categoricals = ["year", "month", "sector", "weekday"]

    # We select 4.5 and one years as the length of our training periods; test periods are one and three months long.
    # Since we are using two years (2015/16) for validation, a one-month test period implies 24 folds.
    train_lengths = [int(4.5 * 252), 252]
    test_lengths = [63, 21]
    test_params = list(product(lookaheads, train_lengths, test_lengths))

    ## Baseline: Linear Regression
    # We always want to know how much our (gradient boosting) is improving over a simpler baseline (if at all..).
    lr = LinearRegression()

    lr_metrics = []
    # iterate over our three CV configuration parameters
    for lookahead, train_length, test_length in tqdm(test_params):
        label = f"r{lookahead:02}_fwd"
        df = pd.get_dummies(
            data.loc[:, features + [label]].dropna(), columns=categoricals, drop_first=True
        )
        X, y = df.drop(label, axis=1), df[label]

        n_splits = int(2 * YEAR / test_length)
        cv = MultipleTimeSeriesCV(
            n_splits=n_splits,
            test_period_length=test_length,
            lookahead=lookahead,
            train_period_length=train_length,
        )

        ic, preds = [], []
        for i, (train_idx, test_idx) in enumerate(cv.split(X=X)):
            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
            X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]
            lr.fit(X_train, y_train)
            y_pred = lr.predict(X_test)
            preds.append(y_test.to_frame("y_true").assign(y_pred=y_pred))
            ic.append(spearmanr(y_test, y_pred)[0])
        preds = pd.concat(preds)
        lr_metrics.append(
            [
                lookahead,
                train_length,
                test_length,
                np.mean(ic),
                spearmanr(preds.y_true, preds.y_pred)[0],
            ]
        )

    columns = ["lookahead", "train_length", "test_length", "ic_by_day", "ic"]
    lr_metrics = pd.DataFrame(lr_metrics, columns=columns)

    ### Information Coefficient - Distribution by Lookahead
    fig, axes = plt.subplots(ncols=2, figsize=(14, 5), sharey=True)

    # plot average of daily IC values
    sns.boxplot(x="lookahead", y="ic_by_day", data=lr_metrics, ax=axes[0])
    axes[0].set_title("IC by Day")

    # plot IC across all predictions
    sns.boxplot(x="lookahead", y="ic", data=lr_metrics, ax=axes[1])
    axes[1].set_title("IC Overall")
    axes[0].set_ylabel("Information Coefficient")
    axes[1].set_ylabel("")
    fig.tight_layout()
    plt.savefig("images/05-01.png")

    ### Best Train/Test Period Lengths
    # For one- and five-day return forecasts, shorter train- and test-length yield better results in terms of daily avg IC:
    print(
        lr_metrics.groupby("lookahead", group_keys=False).apply(
            lambda x: x.nlargest(3, "ic_by_day")
        )
    )
    lr_metrics.to_csv(results_path / "lin_reg_metrics.csv", index=False)

    ## LightGBM Model Tuning
    # The notebook example iterates over many configurations, optionally using random samples to speed up model
    # selection using a diverse subset. The goal is to identify the most impactful parameters without trying every
    # possible combination.
    def get_fi(model):
        """Return normalized feature importance as pd.Series"""
        fi = model.feature_importance(importance_type="gain")
        return pd.Series(fi / fi.sum(), index=model.feature_name())

    ### Hyperparameter Options
    # The `base_params` are not affected by cross-validation:
    base_params = dict(boosting="gbdt", objective="regression", verbose=-1)

    # We choose the following parameters and values to select our best model (see book chapter for detail):
    # constraints on structure (depth) of each tree
    max_depths = [2, 3, 5, 7]
    num_leaves_opts = [2 ** i for i in max_depths]
    min_data_in_leaf_opts = [250, 500, 1000]

    # weight of each new tree in the ensemble
    learning_rate_ops = [0.01, 0.1, 0.3]

    # random feature selection
    feature_fraction_opts = [0.3, 0.6, 0.95]

    param_names = ["learning_rate", "num_leaves", "feature_fraction", "min_data_in_leaf"]
    cv_params = list(
        product(learning_rate_ops, num_leaves_opts, feature_fraction_opts, min_data_in_leaf_opts)
    )
    n_params = len(cv_params)
    print(f"# Parameters: {n_params}")

    ### Train/Test Period Lengths
    lookaheads = [1, 5, 21]
    label_dict = dict(zip(lookaheads, labels))

    # We only use test periods of 63 days length to save some model training and evaluation time.
    train_lengths = [int(4.5 * 252), 252]
    test_lengths = [63]

    test_params = list(product(lookaheads, train_lengths, test_lengths))
    n = len(test_params)
    test_param_sample = np.random.choice(list(range(n)), size=int(n), replace=False)
    test_params = [test_params[i] for i in test_param_sample]
    print("Train configs:", len(test_params))

    ### Categorical Variables
    # We integer-encode categorical variables with values starting at zero, as expected by LightGBM (not necessary
    # as long as the category codes have values less than $2^{32}$, but avoids a warning)
    categoricals = ["year", "weekday", "month"]
    for feature in categoricals:
        data[feature] = pd.factorize(data[feature], sort=True)[0]

    ### Custom Loss Function: Information Coefficient
    def ic_lgbm(preds, train_data):
        """Custom IC eval metric for lightgbm"""
        is_higher_better = True
        return "ic", spearmanr(preds, train_data.get_label())[0], is_higher_better

    ### Run Cross-Validation
    # To explore the hyperparameter space, we specify values for key parameters that we would like to test in
    # combination. The sklearn library supports `RandomizedSearchCV` to cross-validate a subset of parameter
    # combinations that are sampled randomly from specified distributions. We will implement a custom version that
    # allows us to monitor performance, so we can abort the search process once we're satisfied with the result, rather
    # than specifying a set number of iterations beforehand.
    lgb_store = Path(results_path / "tuning_lgb.h5")

    labels = sorted(data.filter(like="fwd").columns)
    features = data.columns.difference(labels).tolist()
    label_dict = dict(zip(lookaheads, labels))

    num_iterations = [10, 25, 50, 75] + list(range(100, 501, 50))
    num_boost_round = num_iterations[-1]

    metric_cols = (
        param_names
        + ["t", "daily_ic_mean", "daily_ic_mean_n", "daily_ic_median", "daily_ic_median_n"]
        + [str(n) for n in num_iterations]
    )

    # We iterate over our six CV configurations and collect the resulting metrics:
    for lookahead, train_length, test_length in test_params:
        # randomized grid search
        cvp = np.random.choice(list(range(n_params)), size=int(n_params / 2), replace=False)
        cv_params_ = [cv_params[i] for i in cvp]

        # set up cross-validation
        n_splits = int(2 * YEAR / test_length)
        print(
            f"Lookahead: {lookahead:2.0f} | "
            f"Train: {train_length:3.0f} | "
            f"Test: {test_length:2.0f} | "
            f"Params: {len(cv_params_):3.0f} | "
            f"Train configs: {len(test_params)}"
        )

        # time-series cross-validation
        cv = MultipleTimeSeriesCV(
            n_splits=n_splits,
            lookahead=lookahead,
            test_period_length=test_length,
            train_period_length=train_length,
        )

        label = label_dict[lookahead]
        outcome_data = data.loc[:, features + [label]].dropna()

        # binary dataset
        lgb_data = lgb.Dataset(
            data=outcome_data.drop(label, axis=1),
            label=outcome_data[label],
            categorical_feature=categoricals,
            free_raw_data=False,
        )
        T = 0
        predictions, metrics, feature_importance, daily_ic = [], [], [], []

        # iterate over (shuffled) hyperparameter combinations
        for p, param_vals in enumerate(cv_params_):
            key = f"{lookahead}/{train_length}/{test_length}/" + "/".join(
                [str(p) for p in param_vals]
            )
            params = dict(zip(param_names, param_vals))
            params.update(base_params)

            start = time()
            cv_preds, nrounds = [], []
            ic_cv = defaultdict(list)

            # iterate over folds
            for i, (train_idx, test_idx) in enumerate(cv.split(X=outcome_data)):

                # select train subset
                lgb_train = lgb_data.subset(
                    used_indices=train_idx.tolist(), params=params
                ).construct()

                # train model for num_boost_round
                model = lgb.train(
                    params=params,
                    train_set=lgb_train,
                    num_boost_round=num_boost_round,
                    verbose_eval=False,
                )
                # log feature importance
                if i == 0:
                    fi = get_fi(model).to_frame()
                else:
                    fi[i] = get_fi(model)

                # capture predictions
                test_set = outcome_data.iloc[test_idx, :]
                X_test = test_set.loc[:, model.feature_name()]
                y_test = test_set.loc[:, label]
                y_pred = {str(n): model.predict(X_test, num_iteration=n) for n in num_iterations}

                # record predictions for each fold
                cv_preds.append(y_test.to_frame("y_test").assign(**y_pred).assign(i=i))

            # combine fold results
            cv_preds = pd.concat(cv_preds).assign(**params)
            predictions.append(cv_preds)

            # compute IC per day
            by_day = cv_preds.groupby(level="date")
            ic_by_day = pd.concat(
                [
                    by_day.apply(lambda x: spearmanr(x.y_test, x[str(n)])[0]).to_frame(n)
                    for n in num_iterations
                ],
                axis=1,
            )
            daily_ic_mean = ic_by_day.mean()
            daily_ic_mean_n = daily_ic_mean.idxmax()
            daily_ic_median = ic_by_day.median()
            daily_ic_median_n = daily_ic_median.idxmax()

            # compute IC across all predictions
            ic = [spearmanr(cv_preds.y_test, cv_preds[str(n)])[0] for n in num_iterations]
            t = time() - start
            T += t

            # collect metrics
            metrics = pd.Series(
                list(param_vals)
                + [
                    t,
                    daily_ic_mean.max(),
                    daily_ic_mean_n,
                    daily_ic_median.max(),
                    daily_ic_median_n,
                ]
                + ic,
                index=metric_cols,
            )
            msg = f'\t{p:3.0f} | {format_time(T)} ({t:3.0f}) | {params["learning_rate"]:5.2f} | '
            msg += f'{params["num_leaves"]:3.0f} | {params["feature_fraction"]:3.0%} | {params["min_data_in_leaf"]:4.0f} | '
            msg += f" {max(ic):6.2%} | {ic_by_day.mean().max(): 6.2%} | {daily_ic_mean_n: 4.0f} | {ic_by_day.median().max(): 6.2%} | {daily_ic_median_n: 4.0f}"
            print(msg)

            # persist results for given CV run and hyperparameter combination
            metrics.to_hdf(lgb_store, "metrics/" + key)
            ic_by_day.assign(**params).to_hdf(lgb_store, "daily_ic/" + key)
            fi.T.describe().T.assign(**params).to_hdf(lgb_store, "fi/" + key)
            cv_preds.to_hdf(lgb_store, "predictions/" + key)

    ## CatBoost Model Tuning
    # We repeat a similar process for CatBoost - see book and CatBoost [docs]
    # (https://catboost.ai/docs/concepts/about.html) for detail.

    ### Hyperparameter Options
    param_names = ["max_depth", "min_child_samples"]

    max_depth_opts = [3, 5, 7, 9]
    min_child_samples_opts = [20, 250, 500]

    cv_params = list(product(max_depth_opts, min_child_samples_opts))
    n_params = len(cv_params)

    ### Train/Test Period Lengths
    lookaheads = [1, 5, 21]
    label_dict = dict(zip(lookaheads, labels))

    train_lengths = [int(4.5 * 252), 252]
    test_lengths = [63]

    test_params = list(product(lookaheads, train_lengths, test_lengths))

    ### Custom Loss Function
    class CatBoostIC(object):
        """Custom IC eval metric for CatBoost"""

        def is_max_optimal(self):
            # Returns whether great values of metric are better
            return True

        def evaluate(self, approxes, target, weight):
            target = np.array(target)
            approxes = np.array(approxes).reshape(-1)
            rho = spearmanr(approxes, target)[0]
            return rho, 1

        def get_final_error(self, error, weight):
            # Returns final value of metric based on error and weight
            return error

    ### Run Cross-Validation
    cb_store = Path(results_path / "tuning_catboost.h5")

    num_iterations = [10, 25, 50, 75] + list(range(100, 1001, 100))
    num_boost_round = num_iterations[-1]

    metric_cols = (
        param_names
        + ["t", "daily_ic_mean", "daily_ic_mean_n", "daily_ic_median", "daily_ic_median_n"]
        + [str(n) for n in num_iterations]
    )

    for lookahead, train_length, test_length in test_params:
        cvp = np.random.choice(list(range(n_params)), size=int(n_params / 1), replace=False)
        cv_params_ = [cv_params[i] for i in cvp]

        n_splits = int(2 * YEAR / test_length)
        print(
            f"Lookahead: {lookahead:2.0f} | Train: {train_length:3.0f} | "
            f"Test: {test_length:2.0f} | Params: {len(cv_params_):3.0f} | Train configs: {len(test_params)}"
        )

        cv = MultipleTimeSeriesCV(
            n_splits=n_splits,
            lookahead=lookahead,
            test_period_length=test_length,
            train_period_length=train_length,
        )

        label = label_dict[lookahead]
        outcome_data = data.loc[:, features + [label]].dropna()
        cat_cols_idx = [outcome_data.columns.get_loc(c) for c in categoricals]
        catboost_data = Pool(
            label=outcome_data[label],
            data=outcome_data.drop(label, axis=1),
            cat_features=cat_cols_idx,
        )
        predictions, metrics, feature_importance, daily_ic = [], [], [], []
        key = f"{lookahead}/{train_length}/{test_length}"
        T = 0
        for p, param_vals in enumerate(cv_params_):
            params = dict(zip(param_names, param_vals))
            # uncomment if running with GPU
            #         params['task_type'] = 'GPU'

            start = time()
            cv_preds, nrounds = [], []
            ic_cv = defaultdict(list)
            for i, (train_idx, test_idx) in enumerate(cv.split(X=outcome_data)):
                train_set = catboost_data.slice(train_idx.tolist())

                model = CatBoostRegressor(**params)
                model.fit(X=train_set, verbose_eval=False)

                test_set = outcome_data.iloc[test_idx, :]
                X_test = test_set.loc[:, model.feature_names_]
                y_test = test_set.loc[:, label]
                y_pred = {str(n): model.predict(X_test, ntree_end=n) for n in num_iterations}
                cv_preds.append(y_test.to_frame("y_test").assign(**y_pred).assign(i=i))

            cv_preds = pd.concat(cv_preds).assign(**params)
            predictions.append(cv_preds)
            by_day = cv_preds.groupby(level="date")
            ic_by_day = pd.concat(
                [
                    by_day.apply(lambda x: spearmanr(x.y_test, x[str(n)])[0]).to_frame(n)
                    for n in num_iterations
                ],
                axis=1,
            )
            daily_ic_mean = ic_by_day.mean()
            daily_ic_mean_n = daily_ic_mean.idxmax()
            daily_ic_median = ic_by_day.median()
            daily_ic_median_n = daily_ic_median.idxmax()

            ic = [spearmanr(cv_preds.y_test, cv_preds[str(n)])[0] for n in num_iterations]
            t = time() - start
            T += t
            metrics = pd.Series(
                list(param_vals)
                + [
                    t,
                    daily_ic_mean.max(),
                    daily_ic_mean_n,
                    daily_ic_median.max(),
                    daily_ic_median_n,
                ]
                + ic,
                index=metric_cols,
            )
            msg = f'{p:3.0f} | {format_time(T)} ({t:3.0f}) | {params["max_depth"]:3.0f} | {params["min_child_samples"]:4.0f} | '
            msg += f" {max(ic):6.2%} | {ic_by_day.mean().max(): 6.2%} | {daily_ic_mean_n: 4.0f} | {ic_by_day.median().max(): 6.2%} | {daily_ic_median_n: 4.0f}"
            print(msg)
            metrics.to_hdf(cb_store, "metrics/" + key)
            ic_by_day.assign(**params).to_hdf(cb_store, "daily_ic/" + key)
            cv_preds.to_hdf(cb_store, "predictions/" + key)
